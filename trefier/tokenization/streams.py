from __future__ import annotations
from typing import Optional, Callable, Iterable, List, Iterator, Tuple, Union
import re

from trefier.misc.location import Range, Position
from trefier.tokenization.latex import LatexParser, Token, Node


class LatexToken:
    def __init__(self, lexeme: str, token_range: Range, envs: Tuple[str, ...]):
        self.lexeme = lexeme
        self.token_range = token_range
        self.envs = envs


class LatexTokenStream:
    def __init__(
        self,
        root: Node,
        lower: bool = True,
        lang: str = 'en',
        words: str = r'''(?:[\w\d_]|(?<!^)\-|\{\\ss\}|\\ss|\\\"(?:a|A|o|O|u|U|s|S))+(?:'s|(?<=n)'t)?|\!|\@|\#|\$|\%|\^|\&|\*|\(|\)|\_|\+|\=|\-|\[|\]|\'|\\|\.|\/|\?|\>|\<|\,|\:|;|\"|\||\{|\}|s+''',
        token_filter: str = r'''\s+|\@|\#|\^|\*|\_|\+|\=|\[|\]|\\|\/|\>|\<|\{|\}''',
        perform_character_replacements: bool = True,
        token_filter_fn: Optional[Callable[[Iterable[Token]], List[bool]]] = None):
        """ Performs more fine graned tokenization on the coarse tokens generated by the latex parser.

        Arguments:
            :param root: The node from which the tokens for the stream are generated.
        
        Keyword Arguments:
            :param lower: Enables lexeme lower() cast.
            :param lang: Language for which the tokens should be generated.
            :param words: Regex that determines what words are.
            :param token_filter: Regex for filtering out unwanted tokens by looking at their lexemes.
            :param perform_character_replacements: If True, replaces latex escaped characters with the correct characters.
            :param token_filter_fn: Filter function applied to the coarse tokens from the provided node.
        """
        self.lower = lower
        # compile the word regex
        self.words = re.compile(words)
        # compile the token fileter regex
        self.token_filter = re.compile(token_filter)
        # determine if german characters should be replaced
        self._perform_german_replacements = perform_character_replacements
        # buffer the tokens of the root
        self.tokens: List[Token] = list(root.tokens)
        if token_filter_fn:
            self.tokens = [
                token
                for token, do_filter
                in zip(self.tokens, token_filter_fn(self.tokens))
                if not do_filter
            ]
        # buffer environments
        self._token_envs = [
            token.envs
            for token in self.tokens
        ]

    def __iter__(self) -> Iterator[LatexToken]:
        lower = str.lower if self.lower else lambda x: x
        for token, envs in zip(self.tokens, self._token_envs):
            if envs and '$' is envs[-1]:
                yield LatexToken(
                    lower(token.lexeme),
                    Range(
                        Position(*token.effective_range[0]),
                        Position(*token.effective_range[1])
                    ),
                    envs)
            else:
                begin = 0
                for split in self.words.finditer(token.lexeme):
                    split_spans = (
                        (begin, split.span()[0]),
                        split.span())
                    for span in split_spans:
                        span_text = token.lexeme[span[0]:span[-1]]
                        if span_text and not self.token_filter.fullmatch(span_text):
                            if self._perform_german_replacements:
                                span_text = _replace_german_characters(span_text)
                            yield LatexToken(
                                lower(span_text),
                                Range(
                                    Position(*token.parser.offset_to_position(token.begin+span[0])),
                                    Position(*token.parser.offset_to_position(token.begin+span[1]))
                                ),
                                envs)
                    begin = split.span()[-1]
                end = len(token.lexeme.rstrip())
                if begin != end:
                    rest_text = token.lexeme[begin:end]
                    if rest_text and not self.token_filter.fullmatch(rest_text):
                        if self._perform_german_replacements:
                            rest_text = _replace_german_characters(rest_text)
                        yield LatexToken(
                            lower(rest_text),
                            Range(
                                Position(*token.parser.offset_to_position(token.begin+begin)),
                                Position(*token.parser.offset_to_position(token.end+end))
                            ),
                            envs)

    @staticmethod
    def from_file(
        file: Union[str, LatexParser],
        lower: bool = True,
        lang: str = 'en',
        perform_character_replacements: bool = True) -> Optional[LatexTokenStream]:
        """ Constructs latex token stream from file or latex parser. Returns None if file failed to parse successfully. """
        if isinstance(file, str):
            file = LatexParser(file)
        assert isinstance(file, LatexParser)
        if not file.success:
            return None
        return LatexTokenStream(
            file.root,
            lang=lang,
            perform_character_replacements=perform_character_replacements,
            lower=lower)


def _replace_german_characters(text: str) -> str:
    return (text.
            replace('\\ss', 'ß').
            replace('\\"s', 'ß').
            replace('\\"a', 'ä').
            replace('\\"A', 'ä').
            replace('\\"u', 'ü').
            replace('\\"U', 'Ü').
            replace('\\"o', 'ö').
            replace('\\"O', 'Ö'))
